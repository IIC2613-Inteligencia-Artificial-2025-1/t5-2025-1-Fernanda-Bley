{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11a799fc",
   "metadata": {},
   "source": [
    "<h1>Actividad 1: Comprendiendo los hiperparámetros (0.5 puntos)</h1>\n",
    "    <section>\n",
    "        <ol>\n",
    "            <li>\n",
    "                Investiga la ecuación de actualización <code>Q(s,a) ← Q(s,a) + α[r + γ max Q(s′,a′) − Q(s,a)]</code>. ¿Cómo explica matemáticamente esta ecuación las diferencias observadas entre ambos agentes? Analiza el impacto específico de <code>α = 0.8</code> versus <code>α = 0.1</code> en la magnitud de las actualizaciones.\n",
    "            </li>\n",
    "            <li>\n",
    "                Investiga cómo funciona la exploración ϵ-greedy y su decaimiento. Si ambos agentes comienzan con <code>ϵ = 1.0</code> y <code>decay = 0.995</code>, ¿en qué episodio aproximadamente cada uno tendría <code>ϵ = 0.1</code>? ¿Cómo interactúa esto con sus diferentes learning rates?\n",
    "            </li>\n",
    "            <li>\n",
    "                ¿Por qué el Agente B experimenta oscilaciones mientras que el Agente A mejora gradualmente? ¿Qué hiperparámetro específico causa esta inestabilidad y cómo?\n",
    "            </li>\n",
    "            <li>\n",
    "                El Agente A con <code>γ = 0.9</code> planifica más hacia el futuro que el Agente B con <code>γ = 0.1</code>. En el contexto de Pong, ¿qué ventajas específicas tendría el Agente A al anticipar la trayectoria de la pelota varios movimientos adelante?\n",
    "            </li>\n",
    "            <li>\n",
    "                Si quisieras combinar lo mejor de ambos agentes (la velocidad inicial del B y la estabilidad del A), ¿qué configuración de hiperparámetros propondrías y por qué? Justifica cómo cada parámetro contribuiría al objetivo deseado.\n",
    "            </li>\n",
    "        </ol>\n",
    "    </section>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2dfd62",
   "metadata": {},
   "source": [
    "\n",
    "<h1>Actividad 3: Experimentación y Análisis del Agente (1.5 puntos)</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5eb20a5",
   "metadata": {},
   "source": [
    "<h2>Experimento 1: El Agente “Miope” vs. el Agente “Previsor” (0.375 puntos)</h2>\n",
    "        <ol>\n",
    "            <li>\n",
    "                Realizar dos entrenamientos: uno con el valor original (<code>DISCOUNT RATE = 0</code>) y otro con un valor alto que incentive al agente a ser “previsor” (por ejemplo, <code>DISCOUNT RATE = 0.95</code>). Entrena cada uno por un número significativo de episodios (ej. 50,000).\n",
    "            </li>\n",
    "            <li>\n",
    "                Genera un gráfico que muestre el <em>mean score</em> (puntaje promedio) a lo largo del tiempo para cada entrenamiento en la misma figura.\n",
    "            </li>\n",
    "            <li>\n",
    "                Analizando el gráfico, responde:\n",
    "                <ul>\n",
    "                    <li>¿Qué agente aprende más rápido al principio?</li>\n",
    "                    <li>¿Cuál alcanza un mejor rendimiento máximo a largo plazo?</li>\n",
    "                    <li>¿Cómo la evidencia visual de tu gráfico demuestra la diferencia fundamental entre un agente que busca recompensas inmediatas y uno que planifica para el futuro?</li>\n",
    "                    <li>En base a esto, ¿qué valor de <code>DISCOUNT RATE</code> elegirías?</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ol>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa8361b",
   "metadata": {},
   "source": [
    "<h2>Experimento 2: El Dilema de la Velocidad vs. la Estabilidad (0.375 puntos)</h2>\n",
    "        <ol>\n",
    "            <li>\n",
    "                Realiza un experimento comparando un LR muy bajo (ej. 0.01) contra un LR muy alto (ej. 0.7).\n",
    "            </li>\n",
    "            <li>\n",
    "                Para cada caso, registra el récord (puntaje récord) y el <em>mean score</em> cada 100 juegos. Presenta tus resultados en una tabla comparativa.\n",
    "            </li>\n",
    "            <li>\n",
    "                Basado en la tabla, compara el proceso de aprendizaje del agente con tasas de aprendizaje (LR) bajas y altas. Describe la velocidad y las características de su rendimiento (por ejemplo, estable, errático, estancamiento) en cada escenario. Finalmente, selecciona el valor de LR que consideres óptimo y justifica tu elección basándote en el compromiso (trade-off) observado entre la velocidad de convergencia y la estabilidad del rendimiento.\n",
    "            </li>\n",
    "        </ol>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d79892",
   "metadata": {},
   "source": [
    "<h2>Experimento 3: Saboteando el Aprendizaje (0.375 puntos)</h2>\n",
    "        <ol>\n",
    "            <li>\n",
    "                Diseña y ejecuta dos experimentos “fallidos”:\n",
    "                <ul>\n",
    "                    <li>a) <strong>Explotación Prematura:</strong> Modifica el EXPLORATION DECAY RATE a un valor muy alto (ej. 0.05) para que el agente deje de explorar casi inmediatamente.</li>\n",
    "                    <li>b) <strong>Exploración Eterna:</strong> Modifica MIN EXPLORATION RATE para que sea igual a MAX EXPLORATION RATE (ej. 0.8), forzando al agente a tomar una acción aleatoria el 80% de las veces, para siempre.</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>\n",
    "                Describe el comportamiento observado en los escenarios de “Explotación Prematura” y “Exploración Eterna”. Evalúa la calidad de las políticas aprendidas y fundamenta cómo el balance entre exploración y explotación en cada caso explica el rendimiento final del agente.\n",
    "            </li>\n",
    "            <li>\n",
    "                Explica cómo los resultados de estos dos experimentos fallidos demuestran por qué es crucial tener una tasa de exploración que decaiga gradualmente con el tiempo.\n",
    "            </li>\n",
    "        </ol>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29e6608",
   "metadata": {},
   "source": [
    "<h2>Experimento 4: Entrenando a tu Agente Ideal (0.375 puntos)</h2>\n",
    "        <ol>\n",
    "            <li>\n",
    "                Selecciona y justifica brevemente la configuración de hiperparámetros que consideres óptima y reentrena al agente con ella.\n",
    "            </li>\n",
    "            <li>\n",
    "                Genera un gráfico que muestre el <em>mean score</em> (puntaje promedio) a lo largo del tiempo del entrenamiento.\n",
    "            </li>\n",
    "            <li>\n",
    "                Reflexiona sobre el rendimiento de tu agente. Considera si un agente “perfecto” es un objetivo realista y describe qué acciones específicas tomarías para mejorar aún más los resultados obtenidos, justificando tu propuesta.\n",
    "            </li>\n",
    "        </ol>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
